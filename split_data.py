import os
import shutil
import time
from pathlib import Path

import pandas as pd
from sklearn.model_selection import train_test_split

from helper_code import get_label, load_header

# ==========================================
# âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ù…Ø³ÛŒØ±Ù‡Ø§ (ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù„ÛŒÙ†ÙˆÚ©Ø³)
# ==========================================
BASE_DIR = Path(".")
DATA_DIR = Path("/home/hadi/Coding/ML/chagas_dataset/python-example-2025")

SOURCE_DIRS = {
    'code15': DATA_DIR / 'code15_output',
    'samitrop': DATA_DIR / 'samitrop_output',
    'ptbxl': DATA_DIR / 'ptbxl_output'
}

# Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
TRAIN_DIR = BASE_DIR / 'training_data'
VAL_DIR = BASE_DIR / 'validation_data'
HOLDOUT_DIR = BASE_DIR / 'holdout_data'

# Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ… Ø¯ÛŒØªØ§
TEST_SIZE = 0.10  # 10% Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ (Holdout)
VAL_SIZE = 0.10   # 10% Ø§Ø² Ú©Ù„ Ø¯ÛŒØªØ§ Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ (Validation)
# (Ù…Ø§Ø¨Ù‚ÛŒ Ú©Ù‡ 80% Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Train) Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)

# ==========================================
# ğŸ›  ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ (Helper Functions)
# ==========================================
def create_symlink(src: Path, dst: Path):
    """Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒÙ†Ú© Ù†Ù…Ø§Ø¯ÛŒÙ† Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹ Ùˆ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯"""
    if dst.exists() or dst.is_symlink():
        dst.unlink()
    os.symlink(src.resolve(), dst)

def clean_directory(dir_path: Path):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø³Ø§Ø®Øª Ù…Ø¬Ø¯Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¯Ø§Ø®Ù„ Ø¯ÛŒØªØ§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
    if dir_path.exists():
        shutil.rmtree(dir_path)
    dir_path.mkdir(parents=True, exist_ok=True)

# ==========================================
# ğŸš€ Ø®Ø· Ù„ÙˆÙ„Ù‡ Ø§ØµÙ„ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ (Main Pipeline)
# ==========================================
def prepare_data_pipeline():
    start_time = time.time()

    print("="*65)
    print("ğŸš€ Starting AI Data Pipeline: Stratified Split & Symlink")
    print("="*65)
    
    # ---------------------------------------------------------
    # ÙØ§Ø² Û±: Ø§Ø³Ú©Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ (Ù…Ø§Ù†Ù†Ø¯ Ú©Ø¯ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§)
    # ---------------------------------------------------------
    print("\nâ³ Phase 1: Scanning files and extracting labels (rglob)...")
    records = []
    
    for source_name, source_path in SOURCE_DIRS.items():
        if not source_path.exists():
            print(f"   âš ï¸ Warning: Source path not found -> {source_path}")
            continue
            
        print(f"   ğŸ” Scanning '{source_name}' recursively...")
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ø± Ø¯Ø± ØªÙ…Ø§Ù…ÛŒ Ø²ÛŒØ±Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
        hea_files = list(source_path.rglob('*.hea'))
        
        for hea_path in hea_files:
            record_base = hea_path.with_suffix('') # Ù…Ø³ÛŒØ± Ø¨Ø¯ÙˆÙ† Ù¾Ø³ÙˆÙ†Ø¯
            try:
                # Ø®ÙˆØ§Ù†Ø¯Ù† Ù‡Ø¯Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒØ¨Ù„ (Ø¬Ù‡Øª Stratified Split)
                header = load_header(str(record_base))
                label = get_label(header)
                
                records.append({
                    'record_name': hea_path.stem,
                    'base_path': record_base,
                    'header_path': hea_path,
                    'label': label,
                    'source': source_name
                })
            except Exception as e:
                print(f"      âš ï¸ Error processing {hea_path.name}: {e}")

    df = pd.DataFrame(records)
    
    if df.empty:
        print("âŒ No records found! Please check your source paths.")
        return

    # --- Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù‚Ø¨Ù„ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… (Pre-Split) ---
    print("\nğŸ“Š Pre-Split Statistics (Total Data Found):")
    print("-" * 45)
    for src, count in df['source'].value_counts().items():
         print(f"   - {src:<15}: {count} records")
    print("-" * 45)
    print(f"   - {'TOTAL':<15}: {len(df)} records")
    
    # ---------------------------------------------------------
    # ÙØ§Ø² Û²: ØªÙ‚Ø³ÛŒÙ… Ù„Ø§ÛŒÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ (Stratified Splits)
    # ---------------------------------------------------------
    print("\nâœ‚ï¸ Phase 2: Performing Stratified Splits (Train/Val/Holdout)...")
    
    # Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„: Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Test (Holdout) Ø§Ø² Ø¨Ù‚ÛŒÙ‡
    train_val_df, holdout_df = train_test_split(
        df, 
        test_size=TEST_SIZE, 
        stratify=df['label'], 
        random_state=42
    )
    
    # Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Val Ø§Ø² Train
    # Ú†ÙˆÙ† test_size Ø§Ø² Ø¯ÛŒØªØ§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø­Ø³Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ù†Ø³Ø¨Øª Ø¢Ù† Ø±Ø§ Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒÙ…
    relative_val_size = VAL_SIZE / (1.0 - TEST_SIZE)
    
    train_df, val_df = train_test_split(
        train_val_df, 
        test_size=relative_val_size, 
        stratify=train_val_df['label'], 
        random_state=42
    )
    
    # --- Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¬Ø¯ÙˆÙ„ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… (Post-Split) ---
    print("\nğŸ“Š Post-Split Statistics:")
    print(f"{'Source':<15} | {'Train':<8} | {'Val':<8} | {'Holdout':<8} | {'Total':<8}")
    print("-" * 65)
    for source in SOURCE_DIRS.keys():
        n_train = len(train_df[train_df['source'] == source])
        n_val = len(val_df[val_df['source'] == source])
        n_hold = len(holdout_df[holdout_df['source'] == source])
        n_total = n_train + n_val + n_hold
        if n_total > 0:
            print(f"{source:<15} | {n_train:<8} | {n_val:<8} | {n_hold:<8} | {n_total:<8}")
    print("-" * 65)
    print(f"{'ALL':<15} | {len(train_df):<8} | {len(val_df):<8} | {len(holdout_df):<8} | {len(df):<8}\n")

    # ---------------------------------------------------------
    # ÙØ§Ø² Û³: Ø³Ø§Ø®Øª Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ùˆ Ù„ÛŒÙ†Ú© Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ (Symlinks)
    # ---------------------------------------------------------
    print("ğŸ“ Phase 3: Cleaning directories and creating Symlinks...")
    
    splits = [
        ('Train', train_df, TRAIN_DIR),
        ('Validation', val_df, VAL_DIR),
        ('Holdout', holdout_df, HOLDOUT_DIR)
    ]
    
    for split_name, split_df, target_dir in splits:
        clean_directory(target_dir) # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù† Ù¾ÙˆØ´Ù‡
        print(f"   ğŸš€ Populating {split_name} data into '{target_dir.name}'...")
        
        count = 0
        for _, row in split_df.iterrows():
            base_src = row['base_path']
            record_name = row['record_name']
            
            # 1. Ù„ÛŒÙ†Ú© ÙØ§ÛŒÙ„ Ù‡Ø¯Ø± (.hea)
            create_symlink(row['header_path'], target_dir / f"{record_name}.hea")
            
            # 2. Ù„ÛŒÙ†Ú© ÙØ§ÛŒÙ„ Ø³ÛŒÚ¯Ù†Ø§Ù„ (.dat)
            dat_src = base_src.with_suffix('.dat')
            if dat_src.exists():
                create_symlink(dat_src, target_dir / f"{record_name}.dat")
                
            # 3. Ù„ÛŒÙ†Ú© ÙØ§ÛŒÙ„ Ù…ØªÙ„Ø¨ (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ .mat)
            mat_src = base_src.with_suffix('.mat')
            if mat_src.exists():
                create_symlink(mat_src, target_dir / f"{record_name}.mat")
                
            count += 1
            if count % 15000 == 0:
                print(f"      ... linked {count} files")
                
        # --- Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ù…ØªØ§Ø¯ÛŒØªØ§ (Best Practice Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø¯Ø§Ø¯Ù‡) ---
        # ÛŒÚ© ÙØ§ÛŒÙ„ CSV Ø¯Ø± Ù‡Ø± Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¨Ø¯Ø§Ù†ÛŒÙ… Ø¯Ù‚ÛŒÙ‚Ø§ Ú†Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ú†Ù‡ Ù„ÛŒØ¨Ù„ÛŒ Ø¯Ø§Ø®Ù„Ø´ Ù‡Ø³ØªÙ†Ø¯
        csv_path = target_dir / f"{split_name.lower()}_metadata.csv"
        # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø³ÛŒØ³ØªÙ… Ø±Ø§ Ø­Ø°Ù Ù…ÛŒÚ©Ù†ÛŒÙ… Ú©Ù‡ ÙÙ‚Ø· Ø¯ÛŒØªØ§ÛŒ ØªÙ…ÛŒØ² Ø¯Ø± CSV Ø¨Ù…Ø§Ù†Ø¯
        clean_df = split_df.drop(columns=['base_path', 'header_path'])
        clean_df.to_csv(csv_path, index=False)

    end_time = time.time()
    duration = end_time - start_time

    print(f"\nâ±ï¸ Total execution time: {int(duration // 60)}m {duration % 60:.2f}s")
    print("\nâœ… Success! The data pipeline is complete. Data is ready for AI training.")
    print("="*65)

if __name__ == '__main__':
    prepare_data_pipeline()